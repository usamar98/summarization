{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textsummarization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc8nB2oBFduQ",
        "outputId": "abe82434-f2dd-4763-f7ab-5f02340819b5"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olR_2Pr6F_XK",
        "outputId": "10138884-e2b8-4962-8e58-b0f6b83a22ea"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from nltk import download\n",
        "download('stopwords')\n",
        "download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "from attention import AttentionLayer\n",
        "pd.set_option(\"display.max_colwidth\", -1)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "fPUwCPqpGtWh",
        "outputId": "bf0c8dc8-a043-4f39-ba89-c4b5beb8413d"
      },
      "source": [
        "reviews = pd.read_csv(\"Book.csv\")\n",
        "print(reviews.shape)\n",
        "reviews.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A few months ago I had the opportunity to complete Andrew Ngs Machine Learning MOOC taught on Coursera It serves as a very good introduction for anyone who wants to venture into the world of AI ML But the catch this course is taught in Octave I always wondered how amazing this course could be if it were in Python I finally decided to re-take the course but only this time I would be completing the programming assignments in Python In these series of blog posts I plan to write about the Python version of the programming exercises used in the course Im doing this for a few reasons It will help anyone who wanted a Python version of the course that includes me as well It will hopefully benefit R users who are willing to learn about the Pythonic implementation of the algorithms they are already familiar with Pre-requisites Its highly reommended that first you watch the week 1 video lectures Should have basic familiarity with the Python ecosystem In this section we will look at the simplest Machine Learning algorithms First some context on the problem statement Here we will implement linear regrssion with one variable to predict profits for a food tuck Suppose you are the CEO of a restaurant franchise ad are considering different cities for opening a new outlet The chain already has trucks in various cities and you have data for profits and populations from the cities The file ex1data txt available under week 2s assignment material contains the dataset for our linear regression exercise The first column is the population of a city and the second column is the profit of a food truck ithat city A negative value for profit indicates a loss</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        text\n",
              "0  A few months ago I had the opportunity to complete Andrew Ngs Machine Learning MOOC taught on Coursera It serves as a very good introduction for anyone who wants to venture into the world of AI ML But the catch this course is taught in Octave I always wondered how amazing this course could be if it were in Python I finally decided to re-take the course but only this time I would be completing the programming assignments in Python In these series of blog posts I plan to write about the Python version of the programming exercises used in the course Im doing this for a few reasons It will help anyone who wanted a Python version of the course that includes me as well It will hopefully benefit R users who are willing to learn about the Pythonic implementation of the algorithms they are already familiar with Pre-requisites Its highly reommended that first you watch the week 1 video lectures Should have basic familiarity with the Python ecosystem In this section we will look at the simplest Machine Learning algorithms First some context on the problem statement Here we will implement linear regrssion with one variable to predict profits for a food tuck Suppose you are the CEO of a restaurant franchise ad are considering different cities for opening a new outlet The chain already has trucks in various cities and you have data for profits and populations from the cities The file ex1data txt available under week 2s assignment material contains the dataset for our linear regression exercise The first column is the population of a city and the second column is the profit of a food truck ithat city A negative value for profit indicates a loss"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3W5XT37HNIj",
        "outputId": "e382f913-f3fe-4ca6-9ad5-d2e56769435e"
      },
      "source": [
        "# Check for any nulls values\n",
        "reviews.isnull().sum()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr22uSGbHY7O",
        "outputId": "99d58928-9034-49a4-982d-035582787157"
      },
      "source": [
        "# let's inspect some reviews\n",
        "for i in range(1):\n",
        "    print(\"Review: \",i)\n",
        "    print(reviews.text)\n",
        "    print('-'*80)\n",
        "    print(reviews.text)\n",
        "    print()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review:  0\n",
            "0    A few months ago I had the opportunity to complete Andrew Ngs Machine Learning MOOC taught on Coursera It serves as a very good introduction for anyone who wants to venture into the world of AI ML But the catch this course is taught in Octave I always wondered how amazing this course could be if it were in Python I finally decided to re-take the course but only this time I would be completing the programming assignments in Python In these series of blog posts I plan to write about the Python version of the programming exercises used in the course Im doing this for a few reasons It will help anyone who wanted a Python version of the course that includes me as well It will hopefully benefit R users who are willing to learn about the Pythonic implementation of the algorithms they are already familiar with Pre-requisites Its highly reommended that first you watch the week 1 video lectures Should have basic familiarity with the Python ecosystem In this section we will look at the simplest Machine Learning algorithms First some context on the problem statement Here we will implement linear regrssion with one variable to predict profits for a food tuck Suppose you are the CEO of a restaurant franchise ad are considering different cities for opening a new outlet The chain already has trucks in various cities and you have data for profits and populations from the cities The file ex1data txt available under week 2s assignment material contains the dataset for our linear regression exercise The first column is the population of a city and the second column is the profit of a food truck ithat city A negative value for profit indicates a loss\n",
            "Name: text, dtype: object\n",
            "--------------------------------------------------------------------------------\n",
            "0    A few months ago I had the opportunity to complete Andrew Ngs Machine Learning MOOC taught on Coursera It serves as a very good introduction for anyone who wants to venture into the world of AI ML But the catch this course is taught in Octave I always wondered how amazing this course could be if it were in Python I finally decided to re-take the course but only this time I would be completing the programming assignments in Python In these series of blog posts I plan to write about the Python version of the programming exercises used in the course Im doing this for a few reasons It will help anyone who wanted a Python version of the course that includes me as well It will hopefully benefit R users who are willing to learn about the Pythonic implementation of the algorithms they are already familiar with Pre-requisites Its highly reommended that first you watch the week 1 video lectures Should have basic familiarity with the Python ecosystem In this section we will look at the simplest Machine Learning algorithms First some context on the problem statement Here we will implement linear regrssion with one variable to predict profits for a food tuck Suppose you are the CEO of a restaurant franchise ad are considering different cities for opening a new outlet The chain already has trucks in various cities and you have data for profits and populations from the cities The file ex1data txt available under week 2s assignment material contains the dataset for our linear regression exercise The first column is the population of a city and the second column is the profit of a food truck ithat city A negative value for profit indicates a loss\n",
            "Name: text, dtype: object\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKpwKAItT85E",
        "outputId": "b8b0ddef-7138-4f39-f7d0-23bbabf935fb"
      },
      "source": [
        "\n",
        "cleaned_text = []\n",
        "for text in reviews['text']:\n",
        "    cleaned_text.append(text)\n",
        "print(\"Texts are complete.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Texts are complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9uDltz5T9UR",
        "outputId": "df869b98-4ab0-40cd-f610-21771c03f491"
      },
      "source": [
        "# let's inspect some clean reviews\n",
        "for i in range(1):\n",
        "    print(\"Review: \",i+1)\n",
        "    print('-'*1)\n",
        "    print(cleaned_text[i])\n",
        "    print()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review:  1\n",
            "-\n",
            "A few months ago I had the opportunity to complete Andrew Ngs Machine Learning MOOC taught on Coursera It serves as a very good introduction for anyone who wants to venture into the world of AI ML But the catch this course is taught in Octave I always wondered how amazing this course could be if it were in Python I finally decided to re-take the course but only this time I would be completing the programming assignments in Python In these series of blog posts I plan to write about the Python version of the programming exercises used in the course Im doing this for a few reasons It will help anyone who wanted a Python version of the course that includes me as well It will hopefully benefit R users who are willing to learn about the Pythonic implementation of the algorithms they are already familiar with Pre-requisites Its highly reommended that first you watch the week 1 video lectures Should have basic familiarity with the Python ecosystem In this section we will look at the simplest Machine Learning algorithms First some context on the problem statement Here we will implement linear regrssion with one variable to predict profits for a food tuck Suppose you are the CEO of a restaurant franchise ad are considering different cities for opening a new outlet The chain already has trucks in various cities and you have data for profits and populations from the cities The file ex1data txt available under week 2s assignment material contains the dataset for our linear regression exercise The first column is the population of a city and the second column is the profit of a food truck ithat city A negative value for profit indicates a loss\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "abUFjOdOULtx",
        "outputId": "bb7d3575-0346-4d5f-9acc-702a8ea0dcf1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_word_count = []\n",
        "\n",
        "\n",
        "for i in cleaned_text:\n",
        "    text_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'text': text_word_count})\n",
        "length_df.hist(bins=15)\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THrLrhTZVd74",
        "outputId": "f0d50f3e-935c-4bfa-ec9b-fa39fe20d37a"
      },
      "source": [
        "count = 0\n",
        "for i in cleaned_text:\n",
        "    if(len(i.split())<=291):\n",
        "        count += 1\n",
        "print(count/len(cleaned_text))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeejxL9RhLK_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "7bf61daf-261e-4808-e51a-ecbaefa2fdc0"
      },
      "source": [
        "\n",
        "max_text_len=250\n",
        "\n",
        "cleaned_text = np.array(cleaned_text)\n",
        "\n",
        "\n",
        "short_text=[]\n",
        "\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    \n",
        "    if(len(cleaned_text[i].split())<=max_text_len):\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_headlines.append(cleaned_headlines[i])\n",
        "\n",
        "df=pd.DataFrame({'text':short_text})\n",
        "x,y=text,text\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_val,y_tr,y_val=train_test_split(x,y,\n",
        "train_size=0.5,\n",
        "test_size=0.5,\n",
        "random_state=123)\n",
        "clf = linear_model.LogisticRegression() \n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-207e95c70102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m random_state=123)\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'linear_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "CuFVpFjrK9Rq",
        "outputId": "6adb649b-4976-4819-bc3b-265c6c0d603e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [text]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdpHcD3FhLYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f284ee-0668-4aa9-d644-182115922f16"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer() \n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "thresh=4\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)\n",
        "\n",
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
        "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding zero upto maximum length\n",
        "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
        "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "#size of vocabulary ( +1 for padding token)\n",
        "x_voc   =  x_tokenizer.num_words + 1\n",
        "\n",
        "x_voc"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 22.22222222222222\n",
            "Total Coverage of rare words: 1.0159651669085632\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czudoI2HhLtZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508e1ce3-e93a-41f0-dfa1-9ce732a7abfb"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer() \n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "thresh=4\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)\n",
        "\n",
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
        "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding zero upto maximum length\n",
        "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
        "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "#size of vocabulary ( +1 for padding token)\n",
        "x_voc   =  x_tokenizer.num_words + 1\n",
        "\n",
        "x_voc"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 22.22222222222222\n",
            "Total Coverage of rare words: 1.0159651669085632\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QutqsFEQFyKk",
        "outputId": "c931c4bf-2d5d-4f62-c032-77136efc26d0"
      },
      "source": [
        "y_tokenizer = Tokenizer()   \n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "thresh=6\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)\n",
        "\n",
        "#prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
        "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
        "\n",
        "#padding zero upto maximum length\n",
        "\n",
        "\n",
        "#size of vocabulary\n",
        "y_voc  =   y_tokenizer.num_words +1\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 29.629629629629626\n",
            "Total Coverage of rare words: 2.1770682148040637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNHHf7zmFUzg"
      },
      "source": [
        "ind=[]\n",
        "for i in range(len(y_tr)):\n",
        "    cnt=0\n",
        "    for j in y_tr[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_tr=np.delete(y_tr,ind, axis=0)\n",
        "x_tr=np.delete(x_tr,ind, axis=0)\n",
        "\n",
        "ind=[]\n",
        "for i in range(len(y_val)):\n",
        "    cnt=0\n",
        "    for j in y_val[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_val=np.delete(y_val,ind, axis=0)\n",
        "x_val=np.delete(x_val,ind, axis=0)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV-EMpOwFfEn",
        "outputId": "e8570f3a-8deb-4a7c-ebe5-58f8a76d7081"
      },
      "source": [
        "\n",
        "from keras import backend as K \n",
        "K.clear_session()\n",
        "\n",
        "latent_dim = 320\n",
        "embedding_dim=220\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len,))\n",
        "\n",
        "#embedding layer\n",
        "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
        "\n",
        "#encoder lstm 1\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "#encoder lstm 2\n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "#encoder lstm 3\n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "#embedding layer\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.3,recurrent_dropout=0.1)\n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention layer\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# Concat attention input and decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "#dense layer\n",
        "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the model \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 250)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 250, 220)     4840        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 250, 320), ( 692480      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 250, 320), ( 820480      lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 220)    4400        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 250, 320), ( 820480      lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 320),  692480      embedding_1[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 320),  205120      lstm_2[0][0]                     \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 640)    0           lstm_3[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 20)     12820       concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 3,253,100\n",
            "Trainable params: 3,253,100\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "fDVm-OF3GgJC",
        "outputId": "3c036a49-1a06-4fd9-87ea-d4177350e0cb"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n",
        "history=model.fit([x_tr,y_tr[:,:-1]],\n",
        "                  y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,\n",
        "epochs=10,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]],\n",
        "y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-2cd84a72dd2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m history=model.fit([x_tr,y_tr[:]],\n\u001b[0;32m----> 4\u001b[0;31m                 \u001b[0my_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m epochs=10,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]],\n\u001b[1;32m      6\u001b[0m y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "UvsW69LGPBYq",
        "outputId": "ed1c75ef-ae7e-4b0e-bdee-65e69b1a5fb3"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-d3f7650c2e80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlCz_Q7sSN0s"
      },
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index\n",
        "\n",
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "      \n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='eostok'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_headlines_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
        "            newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}